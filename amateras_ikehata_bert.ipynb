{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amateras_ikehata_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMGeRAeDqYlG1wp4c87SMvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/modeverv/AI/blob/master/amateras_ikehata_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jdP5Ck2AhZC",
        "colab_type": "text"
      },
      "source": [
        "参照 \n",
        "https://tech-blog.cloud-config.jp/2020-02-06-category-classification-using-bert/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhmsVUoENsRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90ef0d99-b68e-4bda-fd88-bb07b726fff8"
      },
      "source": [
        "# ドライブのマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzGDE4kd9M-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7a9eb6c6-ae65-4de9-b2e2-0b12d881e2c2"
      },
      "source": [
        "!ls /content/drive/My\\ Drive/\n",
        "!ls -la /content/drive/My\\ Drive/bert/data/trains\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 99_other\t      bert\t\t drawio   横地チーム\n",
            " 99_行動記録_コロナ  'Colab Notebooks'\t gyazo\t  社員紹介\n",
            "total 1\n",
            "-rw------- 1 root root 182 Sep 14 05:13 features.csv\n",
            "-rw------- 1 root root  33 Sep 14 05:13 labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhJ7xhep-Lqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f99ff435-015e-4ed0-ead5-44454a82d5c9"
      },
      "source": [
        "# 必要ライブラリのインストール\n",
        "!pip install sentencepiece\n",
        "!pip install keras_bert\n",
        "!pip install np_utils\n",
        "# GPUとかTPUとか使いたい\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "print(tf.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting keras_bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_bert) (1.18.5)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras_bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert) (3.13)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras_bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=e6f7d10da4d0286d04b1b524516fab514dc778a255d3873efc756725f109a453\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=bd689c722bcd969d4b75d00e5b9276270be88bb043bfd4d81dfb629a4b83114c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=84ce67185898fc99824c403e37ea7ec2f30210de7ec4293a215db3a02153c5ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=cfb888f365afe1ed15c7599b2efd71cae438846c77bf81293aba3f834ed22093\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=2ed3dc7604ef12d7ff429a210a0c80bf92444af0d89e5e1c96343ac9e2474cc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=c1f636a1d76a13c9c8a3ec0869c24dabb07a8c94c153a8ffe9bd718892a62e8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=4ee62e4b36659a659e8ee2158dd25edeaeea50a2a982fee3588b1a656056b777\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=062eb3d38e259c0b208192411ef31df3dc4fae8609aebb1f169e4f3c8c99c2d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n",
            "Requirement already satisfied: np_utils in /usr/local/lib/python3.6/dist-packages (0.5.12.1)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.6/dist-packages (from np_utils) (1.18.5)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.6/dist-packages (from np_utils) (0.16.0)\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbK2PAir9ePy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3431608b-183c-419e-fa36-b0c2ffce4efb"
      },
      "source": [
        "# BERTの設定ファイル、モデルのロード\n",
        "# max値を得るプログラム\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "\n",
        "# feature.csvは上記で用意したファイルのパスを指定してください\n",
        "train_features_df = pd.read_csv('/content/drive/My Drive/bert/data/trains/features.csv')\n",
        "\n",
        "def _get_indice(feature):\n",
        "    tokens = []\n",
        "    tokens.append('[CLS]')\n",
        "    tokens.extend(sp.encode_as_pieces(feature))\n",
        "    tokens.append('[SEP]')\n",
        "    number = len(tokens)\n",
        "\n",
        "    return number\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# ダウンロードした事前学習モデルのパスを指定してください\n",
        "sp.Load('/content/drive/My Drive/bert/bert-wiki-ja/wiki-ja.model')\n",
        "\n",
        "numbers = []\n",
        "\n",
        "for feature in train_features_df['feature']:\n",
        "    features_number = _get_indice(feature)\n",
        "    numbers.append(features_number)\n",
        "\n",
        "# 最大トークン数\n",
        "max_token_num = max(numbers)\n",
        "print(\"max_token_number: \" + str(max_token_num))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_token_number: 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhocORbFArX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1594ca19-88fd-4645-833f-940cce3e3003"
      },
      "source": [
        "# 学習データのロード関数\n",
        "import sys\n",
        "sys.path.append('modules')\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "from keras import utils\n",
        "\n",
        "# BERTのロード\n",
        "config_path = '/content/drive/My Drive/bert/bert-wiki-ja/bert_finetuning_config_v1.json'\n",
        "# 拡張子まで記載しない\n",
        "checkpoint_path = '/content/drive/My Drive/bert/bert-wiki-ja/model.ckpt-1400000'\n",
        "\n",
        "# 最大のトークン数\n",
        "SEQ_LEN = 18\n",
        "BATCH_SIZE = 16\n",
        "BERT_DIM = 768\n",
        "LR = 1e-4\n",
        "# 学習回数\n",
        "EPOCH = 20 # 20\n",
        "\n",
        "bert = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True,  trainable=True, seq_len=SEQ_LEN)\n",
        "bert.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 18, 768), (3 24576000    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 18, 768)      1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 18, 768)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 18, 768)      13824       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 18, 768)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 18, 768)      1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 18, 768)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 18, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 18, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 18, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 18, 768)      0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 18, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 18, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 18, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 18, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 18, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 18, 768)      0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 18, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 18, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 18, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 18, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 18, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 18, 768)      0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 18, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 18, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, 18, 768)      590592      Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, 18, 768)      1536        MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Sim (EmbeddingSimilarity)   (None, 18, 32000)    32000       MLM-Norm[0][0]                   \n",
            "                                                                 Embedding-Token[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Masked (InputLayer)       [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "MLM (Masked)                    (None, 18, 32000)    0           MLM-Sim[0][0]                    \n",
            "                                                                 Input-Masked[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 110,863,618\n",
            "Trainable params: 110,863,618\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK_b_hy2IGNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習データのロード関数\n",
        "from keras import utils\n",
        "import numpy as np # これ必要だったよ？\n",
        "\n",
        "maxlen = SEQ_LEN # maxlenがundefinedだよ？\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('/content/drive/My Drive/bert/bert-wiki-ja/wiki-ja.model')\n",
        "\n",
        "def _get_indice(feature):\n",
        "    indices = np.zeros((maxlen), dtype = np.int32)\n",
        "\n",
        "    tokens = []\n",
        "    tokens.append('[CLS]')\n",
        "    tokens.extend(sp.encode_as_pieces(feature))\n",
        "    tokens.append('[SEP]')\n",
        "\n",
        "    for t, token in enumerate(tokens):\n",
        "        if t >= maxlen:\n",
        "            break\n",
        "        try:\n",
        "            indices[t] = sp.piece_to_id(token)\n",
        "        except:\n",
        "            logging.warn(f'{token} is unknown.')\n",
        "            indices[t] = sp.piece_to_id('<unk>')\n",
        "\n",
        "    return indices\n",
        "\n",
        "def _load_labeldata(train_dir, test_dir):\n",
        "    train_features_df = pd.read_csv(f'{train_dir}/features.csv')\n",
        "    train_labels_df = pd.read_csv(f'{train_dir}/labels.csv')\n",
        "    test_features_df = pd.read_csv(f'{test_dir}/features.csv')\n",
        "    test_labels_df = pd.read_csv(f'{test_dir}/labels.csv')\n",
        "    label2index = {k: i for i, k in enumerate(train_labels_df['label'].unique())}\n",
        "    index2label = {i: k for i, k in enumerate(train_labels_df['label'].unique())}\n",
        "    class_count = len(label2index)\n",
        "    train_labels = utils.np_utils.to_categorical([label2index[label] for label in train_labels_df['label']], num_classes=class_count)\n",
        "    test_label_indices = [label2index[label] for label in test_labels_df['label']]\n",
        "    test_labels = utils.np_utils.to_categorical(test_label_indices, num_classes=class_count)\n",
        "\n",
        "    train_features = []\n",
        "    test_features = []\n",
        "\n",
        "    for feature in train_features_df['feature']:\n",
        "        train_features.append(_get_indice(feature))\n",
        "    train_segments = np.zeros((len(train_features), maxlen), dtype = np.float32)\n",
        "    print(\"maxlen\")\n",
        "    print(maxlen)\n",
        "    for feature in test_features_df['feature']:\n",
        "        test_features.append(_get_indice(feature))\n",
        "    test_segments = np.zeros((len(test_features), maxlen), dtype = np.float32)\n",
        "\n",
        "    print(f'Trainデータ数: {len(train_features_df)}, Testデータ数: {len(test_features_df)}, ラベル数: {class_count}')\n",
        "\n",
        "    return {\n",
        "        'class_count': class_count,\n",
        "        'label2index': label2index,\n",
        "        'index2label': index2label,\n",
        "        'train_labels': train_labels,\n",
        "        'test_labels': test_labels,\n",
        "        'test_label_indices': test_label_indices,\n",
        "        'train_features': np.array(train_features),\n",
        "        'train_segments': np.array(train_segments),\n",
        "        'test_features': np.array(test_features),\n",
        "        'test_segments': np.array(test_segments),\n",
        "        'input_len': maxlen\n",
        "    }"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XziaTjhEFfgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル作成関数\n",
        "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Flatten, GlobalMaxPooling1D\n",
        "from keras_bert.layers import MaskedGlobalMaxPool1D\n",
        "from keras import Input, Model\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "\n",
        "def _create_model(input_shape, class_count):\n",
        "    decay_steps, warmup_steps = calc_train_steps(\n",
        "        input_shape[0],\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCH,\n",
        "    )\n",
        "\n",
        "    bert_last = bert.get_layer(name='NSP-Dense').output\n",
        "    x1 = bert_last\n",
        "    output_tensor = Dense(class_count, activation='softmax')(x1)\n",
        "    # Trainableの場合は、Input Masked Layerが3番目の入力なりますが、\n",
        "    # FineTuning時には必要無いので1, 2番目の入力だけ使用します。\n",
        "    # Trainableでなければkeras-bertのModel.inputそのままで問題ありません。\n",
        "    model = Model([bert.input[0], bert.input[1]], output_tensor)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=LR),\n",
        "                  #optimizer='nadam',\n",
        "                  metrics=['mae', 'mse', 'acc'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-khbmdBMXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10aafc52-659d-4678-844e-aeb909c7dd3e"
      },
      "source": [
        "# 学習データのロードとモデルの準備\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "from keras import utils\n",
        "\n",
        "trains_dir = '/content/drive/My Drive/bert/data/trains'\n",
        "tests_dir = '/content/drive/My Drive/bert/data/tests'\n",
        "\n",
        "data = _load_labeldata(trains_dir, tests_dir)\n",
        "model_filename = '/content/drive/My Drive/bert/models/knbc_finetuning.model'\n",
        "model = _create_model(data['train_features'].shape, data['class_count'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maxlen\n",
            "18\n",
            "Trainデータ数: 2, Testデータ数: 2, ラベル数: 2\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 18)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 18, 768), (3 24576000    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 18, 768)      1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 18, 768)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 18, 768)      13824       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 18, 768)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 18, 768)      1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 18, 768)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 18, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 18, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 18, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 18, 768)      0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 18, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 18, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 18, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 18, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 18, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 18, 768)      0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 18, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 18, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 18, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 18, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 18, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 18, 768)      0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 18, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 18, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 18, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 18, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 18, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 18, 768)      0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 18, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 18, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            1538        NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 110,239,490\n",
            "Trainable params: 110,239,490\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lzzizubaxDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1b693917-ca42-4aa9-8b8b-f32684c27c67"
      },
      "source": [
        "print(data)\n",
        "#!gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'class_count': 2, 'label2index': {'スポーツ': 0, '携帯電話': 1}, 'index2label': {0: 'スポーツ', 1: '携帯電話'}, 'train_labels': array([[1., 0.],\n",
            "       [0., 1.]], dtype=float32), 'test_labels': array([[1., 0.],\n",
            "       [0., 1.]], dtype=float32), 'test_label_indices': [0, 1], 'train_features': array([[    4,  4570,  4747,   162,    10,  4538,    11,   619,    63,\n",
            "           92, 10636,   488,    12, 17862, 25046,   237,    27,     5],\n",
            "       [    4,     9,  1778, 15448,  1035, 10301,   858,  8688,    10,\n",
            "          977,    10, 16744,    95,  3017,     5,     0,     0,     0]],\n",
            "      dtype=int32), 'train_segments': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]], dtype=float32), 'test_features': array([[    4,  4570,   488,    12, 17862, 25046,   237,    27,     5,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "       [    4,     9,  1778,    12, 18695,     5,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "      dtype=int32), 'test_segments': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.]], dtype=float32), 'input_len': 18}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adVf2O5WBnrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "24d43a95-0e52-49bf-f0ea-3141658930bb"
      },
      "source": [
        "# 学習の実行\n",
        "history = model.fit([data['train_features'], data['train_segments']],\n",
        "          data['train_labels'],\n",
        "          epochs = EPOCH,\n",
        "          batch_size = BATCH_SIZE,\n",
        "          validation_data=([data['test_features'], data['test_segments']], data['test_labels']),\n",
        "          shuffle=False,\n",
        "          verbose = 1,\n",
        "          callbacks = [\n",
        "              ModelCheckpoint(monitor='val_acc', mode='max', filepath=model_filename, save_best_only=True)\n",
        "          ])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9856 - mae: 0.5700 - mse: 0.3705 - acc: 0.5000WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/bert/models/knbc_finetuning.model/assets\n",
            "1/1 [==============================] - 36s 36s/step - loss: 0.9856 - mae: 0.5700 - mse: 0.3705 - acc: 0.5000 - val_loss: 0.4456 - val_mae: 0.3595 - val_mse: 0.1293 - val_acc: 1.0000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2423 - mae: 0.2147 - mse: 0.0469 - acc: 1.0000 - val_loss: 0.4136 - val_mae: 0.3031 - val_mse: 0.1402 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0416 - mae: 0.0408 - mse: 0.0017 - acc: 1.0000 - val_loss: 0.4841 - val_mae: 0.3222 - val_mse: 0.1835 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0091 - mae: 0.0091 - mse: 8.3475e-05 - acc: 1.0000 - val_loss: 0.5307 - val_mae: 0.3345 - val_mse: 0.2088 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0076 - mae: 0.0075 - mse: 5.6979e-05 - acc: 1.0000 - val_loss: 0.5524 - val_mae: 0.3392 - val_mse: 0.2204 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0046 - mae: 0.0046 - mse: 2.1415e-05 - acc: 1.0000 - val_loss: 0.5601 - val_mae: 0.3401 - val_mse: 0.2249 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0030 - mae: 0.0030 - mse: 1.0064e-05 - acc: 1.0000 - val_loss: 0.5555 - val_mae: 0.3377 - val_mse: 0.2234 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - mae: 0.0019 - mse: 3.8001e-06 - acc: 1.0000 - val_loss: 0.5495 - val_mae: 0.3351 - val_mse: 0.2211 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.1333e-04 - mae: 7.1308e-04 - mse: 5.0870e-07 - acc: 1.0000 - val_loss: 0.5427 - val_mae: 0.3325 - val_mse: 0.2184 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.5646e-04 - mae: 4.5634e-04 - mse: 2.3174e-07 - acc: 1.0000 - val_loss: 0.5362 - val_mae: 0.3301 - val_mse: 0.2156 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.6322e-04 - mae: 3.6315e-04 - mse: 1.4606e-07 - acc: 1.0000 - val_loss: 0.5308 - val_mae: 0.3280 - val_mse: 0.2132 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.3975e-04 - mae: 5.3962e-04 - mse: 2.9789e-07 - acc: 1.0000 - val_loss: 0.5258 - val_mae: 0.3262 - val_mse: 0.2111 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.0996e-04 - mae: 2.0993e-04 - mse: 4.5622e-08 - acc: 1.0000 - val_loss: 0.5218 - val_mae: 0.3247 - val_mse: 0.2093 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.9029e-04 - mae: 2.9025e-04 - mse: 8.4345e-08 - acc: 1.0000 - val_loss: 0.5187 - val_mae: 0.3235 - val_mse: 0.2079 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.6813e-04 - mae: 1.6814e-04 - mse: 2.8296e-08 - acc: 1.0000 - val_loss: 0.5164 - val_mae: 0.3227 - val_mse: 0.2069 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 1.7969e-04 - mae: 1.7968e-04 - mse: 3.4162e-08 - acc: 1.0000 - val_loss: 0.5148 - val_mae: 0.3221 - val_mse: 0.2062 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 2.7444e-04 - mae: 2.7440e-04 - mse: 8.6172e-08 - acc: 1.0000 - val_loss: 0.5138 - val_mae: 0.3217 - val_mse: 0.2057 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 2.5144e-04 - mae: 2.5138e-04 - mse: 6.6679e-08 - acc: 1.0000 - val_loss: 0.5132 - val_mae: 0.3215 - val_mse: 0.2054 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.2320e-04 - mae: 1.2320e-04 - mse: 1.5403e-08 - acc: 1.0000 - val_loss: 0.5129 - val_mae: 0.3214 - val_mse: 0.2053 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.8338e-04 - mae: 1.8336e-04 - mse: 3.8195e-08 - acc: 1.0000 - val_loss: 0.5129 - val_mae: 0.3214 - val_mse: 0.2053 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzQePJFwIe4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "6e24172f-409a-4523-a0d9-70cf40707956"
      },
      "source": [
        "df = pd.DataFrame(history.history)\n",
        "display(df)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>mae</th>\n",
              "      <th>mse</th>\n",
              "      <th>acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_mae</th>\n",
              "      <th>val_mse</th>\n",
              "      <th>val_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.985641</td>\n",
              "      <td>0.570027</td>\n",
              "      <td>3.705293e-01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.445565</td>\n",
              "      <td>0.359532</td>\n",
              "      <td>0.129270</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.242337</td>\n",
              "      <td>0.214711</td>\n",
              "      <td>4.688192e-02</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.413619</td>\n",
              "      <td>0.303148</td>\n",
              "      <td>0.140246</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.041628</td>\n",
              "      <td>0.040772</td>\n",
              "      <td>1.664603e-03</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.484121</td>\n",
              "      <td>0.322187</td>\n",
              "      <td>0.183485</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.009095</td>\n",
              "      <td>0.009053</td>\n",
              "      <td>8.347549e-05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.530710</td>\n",
              "      <td>0.334473</td>\n",
              "      <td>0.208834</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.007576</td>\n",
              "      <td>0.007547</td>\n",
              "      <td>5.697853e-05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.552353</td>\n",
              "      <td>0.339151</td>\n",
              "      <td>0.220437</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.004617</td>\n",
              "      <td>0.004607</td>\n",
              "      <td>2.141487e-05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.560070</td>\n",
              "      <td>0.340137</td>\n",
              "      <td>0.224878</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.003022</td>\n",
              "      <td>0.003017</td>\n",
              "      <td>1.006385e-05</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.555508</td>\n",
              "      <td>0.337715</td>\n",
              "      <td>0.223449</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001929</td>\n",
              "      <td>0.001927</td>\n",
              "      <td>3.800122e-06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.549490</td>\n",
              "      <td>0.335141</td>\n",
              "      <td>0.221146</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>5.087019e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.542704</td>\n",
              "      <td>0.332496</td>\n",
              "      <td>0.218351</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>2.317388e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.536242</td>\n",
              "      <td>0.330057</td>\n",
              "      <td>0.215604</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>1.460579e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.530766</td>\n",
              "      <td>0.328007</td>\n",
              "      <td>0.213238</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>2.978903e-07</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.525794</td>\n",
              "      <td>0.326163</td>\n",
              "      <td>0.211056</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>4.562188e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.521833</td>\n",
              "      <td>0.324692</td>\n",
              "      <td>0.209304</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>8.434463e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.518746</td>\n",
              "      <td>0.323544</td>\n",
              "      <td>0.207931</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>2.829603e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.516392</td>\n",
              "      <td>0.322667</td>\n",
              "      <td>0.206880</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>3.416167e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.514764</td>\n",
              "      <td>0.322058</td>\n",
              "      <td>0.206151</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.000274</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>8.617238e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.513785</td>\n",
              "      <td>0.321688</td>\n",
              "      <td>0.205715</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>6.667922e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.513161</td>\n",
              "      <td>0.321453</td>\n",
              "      <td>0.205435</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>1.540329e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.512889</td>\n",
              "      <td>0.321351</td>\n",
              "      <td>0.205314</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>3.819518e-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.512889</td>\n",
              "      <td>0.321351</td>\n",
              "      <td>0.205314</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        loss       mae           mse  ...   val_mae   val_mse  val_acc\n",
              "0   0.985641  0.570027  3.705293e-01  ...  0.359532  0.129270      1.0\n",
              "1   0.242337  0.214711  4.688192e-02  ...  0.303148  0.140246      0.5\n",
              "2   0.041628  0.040772  1.664603e-03  ...  0.322187  0.183485      0.5\n",
              "3   0.009095  0.009053  8.347549e-05  ...  0.334473  0.208834      0.5\n",
              "4   0.007576  0.007547  5.697853e-05  ...  0.339151  0.220437      0.5\n",
              "5   0.004617  0.004607  2.141487e-05  ...  0.340137  0.224878      0.5\n",
              "6   0.003022  0.003017  1.006385e-05  ...  0.337715  0.223449      0.5\n",
              "7   0.001929  0.001927  3.800122e-06  ...  0.335141  0.221146      0.5\n",
              "8   0.000713  0.000713  5.087019e-07  ...  0.332496  0.218351      0.5\n",
              "9   0.000456  0.000456  2.317388e-07  ...  0.330057  0.215604      0.5\n",
              "10  0.000363  0.000363  1.460579e-07  ...  0.328007  0.213238      0.5\n",
              "11  0.000540  0.000540  2.978903e-07  ...  0.326163  0.211056      0.5\n",
              "12  0.000210  0.000210  4.562188e-08  ...  0.324692  0.209304      0.5\n",
              "13  0.000290  0.000290  8.434463e-08  ...  0.323544  0.207931      0.5\n",
              "14  0.000168  0.000168  2.829603e-08  ...  0.322667  0.206880      0.5\n",
              "15  0.000180  0.000180  3.416167e-08  ...  0.322058  0.206151      0.5\n",
              "16  0.000274  0.000274  8.617238e-08  ...  0.321688  0.205715      0.5\n",
              "17  0.000251  0.000251  6.667922e-08  ...  0.321453  0.205435      0.5\n",
              "18  0.000123  0.000123  1.540329e-08  ...  0.321351  0.205314      0.5\n",
              "19  0.000183  0.000183  3.819518e-08  ...  0.321351  0.205314      0.5\n",
              "\n",
              "[20 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq1xmt5IIzyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2ab251d9-8f1f-4a93-91c5-2570a8e04b91"
      },
      "source": [
        "# モデルの評価\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.models import load_model\n",
        "from keras_bert import get_custom_objects\n",
        "\n",
        "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
        "\n",
        "predicted_test_labels = model.predict([data['test_features'], data['test_segments']]).argmax(axis=1)\n",
        "numeric_test_labels = np.array(data['test_labels']).argmax(axis=1)\n",
        "\n",
        "report = classification_report(\n",
        "        numeric_test_labels, predicted_test_labels, target_names=['スポーツ','携帯電話'], output_dict=True)\n",
        "display(pd.DataFrame(report).T)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>スポーツ</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>携帯電話</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>macro avg</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted avg</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              precision  recall  f1-score  support\n",
              "スポーツ                1.0     1.0       1.0      1.0\n",
              "携帯電話                1.0     1.0       1.0      1.0\n",
              "accuracy            1.0     1.0       1.0      1.0\n",
              "macro avg           1.0     1.0       1.0      2.0\n",
              "weighted avg        1.0     1.0       1.0      2.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrMX3IjwI_Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 予測\n",
        "import sys\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "from keras import utils\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "from keras_bert import get_custom_objects\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "sys.path.append('modules')\n",
        "\n",
        "# SentencePieceProccerモデルの読込\n",
        "spp = spm.SentencePieceProcessor()\n",
        "spp.Load('/content/drive/My Drive/bert/bert-wiki-ja/wiki-ja.model')\n",
        "# BERTの学習したモデルの読込\n",
        "model_filename = '/content/drive/My Drive/bert/models/knbc_finetuning.model'\n",
        "model = load_model(model_filename, custom_objects=get_custom_objects())\n",
        "\n",
        "SEQ_LEN = 18\n",
        "maxlen = SEQ_LEN\n",
        "\n",
        "def _get_indice(feature):\n",
        "    indices = np.zeros((maxlen), dtype=np.int32)\n",
        "\n",
        "    tokens = []\n",
        "    tokens.append('[CLS]')\n",
        "    tokens.extend(spp.encode_as_pieces(feature))\n",
        "    tokens.append('[SEP]')\n",
        "\n",
        "    for t, token in enumerate(tokens):\n",
        "        if t >= maxlen:\n",
        "            break\n",
        "        try:\n",
        "            indices[t] = spp.piece_to_id(token)\n",
        "        except:\n",
        "            logging.warn('unknown')\n",
        "            indices[t] = spp.piece_to_id('<unk>')\n",
        "    return indices\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtIwxX943S7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fadfdff-e194-46b5-8e78-00630029d5a2"
      },
      "source": [
        "# 予測実行\n",
        "feature = \"スポーツ\"\n",
        "\n",
        "test_features = []\n",
        "test_features.append(_get_indice(feature))\n",
        "test_segments = np.zeros(\n",
        "    (len(test_features), maxlen), dtype=np.float32)\n",
        "\n",
        "predicted_test_labels = model.predict(\n",
        "    [np.array(test_features), test_segments]).argmax(axis=1)\n",
        "label_data = pd.read_csv('/content/drive/My Drive/bert/label_id/id_category.csv')\n",
        "label = label_data.query(f'id == {predicted_test_labels[0]}')\n",
        "label = label.iloc[0]\n",
        "label_name = label['label']\n",
        "print(label_name)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "携帯電話\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}